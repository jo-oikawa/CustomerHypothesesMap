# Jobs To Be Done

Each job expresses situational progress the user seeks, including the desired outcome.

- J1 Project orientation – When opening or returning to a repo, rapidly form a mental map (architecture, build system, key flows) to start contributing confidently.
- J2 Task scoping – When picking up an issue, identify all relevant code & dependencies to estimate effort and plan safe changes.
- J3 Environment readiness – When starting work, have tools, dependencies, build & debug configs function immediately without yak shaving.
- J4 Code navigation – When following logic, jump precisely (defs, refs, types, tests) with minimal latency to sustain reasoning flow.
- J5 Code comprehension – When reading unfamiliar code, distill intent, contracts, and side effects to reduce ramp time.
- J6 Modification & implementation with guardrails – While editing, receive inline, early signals (types, lint, semantic checks) to prevent defects.
- J7 Boilerplate & repetition reduction – When encountering repetitive patterns, generate or template them safely to save time.
- J8 Refactoring – When improving structure, perform confident, atomic transformations preserving behavior.
- J9 Debugging & diagnostics – When facing a failure, trace the shortest path from symptom to root cause with contextual clues.
- J10 Testing – When writing or changing code, create and run relevant tests quickly to validate correctness.
- J11 Dependency & build insight – When builds slow or fail, surface bottlenecks and configuration issues for targeted fixes.
- J12 Performance profiling – When code is slow, attribute time/cost to specific hot paths or resource usage to guide optimization.
- J13 Code review readiness – Before submitting changes, validate style, risk, and impact to streamline review cycles.
- J14 Collaboration & handoff – When others need context, share concise artifacts (diff rationale, assumptions) for smooth transition.
- J15 Knowledge capture – When solving non-obvious problems, document insight in-place to benefit future work.
- J16 Multi-repo / mono-repo management – When spanning services/packages, navigate and assess cross-impact to avoid regressions.
- J17 Tech & API migration – When versions/frameworks evolve, perform guided updates with minimized risk and churn.
- J18 Security & compliance checks – While coding, detect vulnerable or non-compliant patterns early to prevent downstream risk.
- J19 Workflow automation – When manual sequences repeat, script or extend the editor so future runs are one step.
- J20 Personal growth – When encountering gaps, access concise learning in context without breaking flow.

Specific to Designer / PM / Non-traditional Developer:
- J21 Concept-to-code mapping – When finalizing design decisions, locate implementation points to clarify ownership and feasibility.
- J22 Early feasibility probing – When proposing interactions, test constraints quickly to refine scope.
- J23 Change impact storytelling – When informing stakeholders, assemble a narrative linking rationale, code changes, and risks.
- J24 Spec compliance tracing – When reviewing builds, trace implementation back to requirements to catch drift.

Specific to Eager AI Adopter:
- J25 AI orchestration – When chaining tasks (generate→refactor→test), define reliable, low-friction multi-step AI flows.
- J26 Context curation – When invoking AI on large codebases, select and prune the most relevant context to improve accuracy.
- J27 Outcome evaluation – When applying AI suggestions, measure impact (defects, time saved, review friction) to validate value.
- J28 Policy & boundary enforcement – When automating, ensure changes respect architectural, style, and compliance constraints.
- J29 Multi-agent delegation – When a feature spans concerns, coordinate specialized AI roles and reconcile outputs.
- J30 Knowledge distillation – When exploring a new domain, extract patterns, invariants, and risks into living documentation.
- J31 Safe bulk change execution – When applying widespread edits, stage, cluster, and validate diffs before merging.
- J32 Model/version experimentation – When new models appear, compare quality, latency, and hallucination rates on representative tasks.
